\section{Équations maîtres}
\subsection{Équation de Schrödinger et opérateur d'évolution}
L'évolution temporelle d'un système quantique est décrit par la célèbre équation de Schrödinger où on pose ici $\hbar = 1$ :

\begin{equation}
    i\derivative{}{t}\ket{\psi(t)} = H(t)\ket{\psi(t)}
\end{equation}

En définissant un opérateur d'évolution $U(t,t_0)$ qui amène un état du temps $t_0$ au temps $t$, c'est-à-dire que $\ket{\psi(t)} = U(t,t_0)\ket{\psi(t_0)}$, on peut l'injecter dans (1) afin d'avoir 

\begin{equation}
    i \derivative{}{t}U(t,t_0)\ket{\psi(t_0)} = H(t)U(t,t_0)\ket{\psi(t_0)} \implies i \derivative{}{t}U(t,t_0) = H(t)U(t,t_0)
\end{equation}

Par ailleurs, il va de soi que $U(t,t) = \mathbb{I}$ pour tout temps $t$, car il n'y a alors, selon notre définition, aucune évolution qui a lieu. Dans ce cas, comme rien ne se passe, il faut que l'opérateur d'évolution soit l'identité. Aussi, on peut décomposer une évolution $U(t, t_0)$ en plusieurs évolutions une à la suite de l'autre de manière à avoir $U(t,t_0) = U(t,t_1)U(t_1,t_0)$ par exemple (il pourrait y en avoir autant qu'on veut).

\subsection{Système fermé et hamiltonien statique}
Pour un système fermé et dont l'hamiltonien est statique (donc indépendant du temps), (1.2) devient

\begin{equation}
    i\derivative{}{t}U(t,t_0) = HU(t,t_0) \implies \int_{U(t_0,t_0)}^{U(t,t_0)}\frac{\diff U^{'}}{U'} = -iH \int_{t_0}^{t}dt^{'} \implies U(t,t_0) = e^{-iH(t-t_0)}
\end{equation}

\subsection{Système fermé et hamiltonien dynamique}
Sachant la forme de l'opérateur d'évolution lorsque l'hamiltonien est constant, on peut approximer l'opérateur d'évolution dans le cas où l'hamiltonien est dynamique. En effet, pour une variation infinitésimale de temps $\delta t$, l'hamiltonien change à peine de sa forme de départ et on peut le considérer comme étant constant sur ce petit intervalle. Lorsqu'on dit constant, on veut plutôt dire que l'hamiltonien est évalué à un point fixe à quelque part dans le petit intervalle de temps et ce pour toute la durée de l'opérateur d'évolution. Ainsi, 

\begin{equation*}
    U(t+\delta t, t) \approx e^{-iH(t)\delta t}
\end{equation*}

Ici, on évalue l'hamiltonien à $t$, car de toute manière $\delta \rightarrow 0$ ce qui en fait un choix pratique. Cependant, on aimerait avoir une forme explicite pour l'opérateur d'évolution sur une plus grande période de temps. On sait qu'on peut découper l'évolution, par exemple, en $N$ sous-intervalles égaux de temps $\epsilon = \frac{t-t_0}{N}$ pour que 

\begin{equation*}
    U(t,t_0) = \prod_{k=1}^{N}U(t_0 + k\epsilon, t_0 + (k-1)\epsilon)
\end{equation*}

Dans la limite où $\epsilon \rightarrow 0$ (donc où $N \rightarrow \infty$), l'approximation plus haut est valide (les sous-intervalles deviennent infiniment petits) et dès lors on peut se dire que

\begin{equation*}
    U(t_0 + k\epsilon, t_0 + (k-1)\epsilon) \approx e^{-i\epsilon H\left(t_0 + (k-1)\epsilon\right)}
\end{equation*}

Alors,

\begin{equation*}
    U(t,t_0) = \lim_{\epsilon \rightarrow 0} \prod_{k=1}^{N}U(t_0 + k\epsilon, t_0 + (k-1)\epsilon) = \lim_{\epsilon \rightarrow 0} \prod_{k=1}^{N} e^{-i\epsilon H(t_0 + (k-1)\epsilon)} = \lim_{\epsilon \rightarrow 0} e^{-i\epsilon H(t_0)}e^{-i\epsilon H(t_0 + \epsilon)} ... e^{-i\epsilon H(t-\epsilon)} 
\end{equation*}

On serait très tenté de combiner les exponentielles en sommant leur argument, mais on travaille avec des matrices et il faut alors faire attention. Effectivement, on peut combiner deux exponentielles contenant une matrice uniquement lorsque ces matrices commutent. Autrement, il faudrait utiliser la formule de Baker-Campbell-Hausdorff qui ne semble pas nous faire progresser avec son infinité de termes. En général, avec les hamiltoniens, cela ne sera pas le cas. On s'attarde alors aux deux cas possibles.

\subsubsection{Les hamiltoniens commutent}

D'abord, en supposant que tous les $H(t_i)$ dans les exponentielles commutent entre eux, donc que $\left[H(t_i), H(t_j)\right] = 0 \ \forall i,j$, il est possible de combiner toutes les exponentielles de la manière suivante :

\begin{equation}
    U(t,t_0) = \lim_{\epsilon \rightarrow 0} e^{-i \sum_{k=0}^{N-1}H(t_0 + k\epsilon)\epsilon} = e^{-i\int_{t_0}^{t}H(t^{'})dt^{'}}
\end{equation}

On voit clairement que si l'hamiltonien est constant, alors il peut être sorti de l'intégrale redonnant ainsi (1.3).

\subsubsection{Les hamiltoniens ne commutent pas}
Pour ce cas, on utilise une approche itérative. Depuis (1.2) et par une approche similaire à (1.3), on peut écrire 

\begin{equation*}
    \int_{U(t_0,t_0)}^{U(t,t_0)}dU^{'} = -i \int_{t_0}^{t}H(t^{'})U(t^{'}, t_0)dt^{'} \implies U(t,t_0) = \mathbb{I} -i\int_{t_0}^{t}H(t^{'})U(t^{'},t_0)dt^{'}
\end{equation*}

Il ne s'agit pas d'une solution, car on trouve $U(t,t_0)$ des deux côtés de l'équation. Par contre, avec ce fait, on peut remplacer l'équation dans elle-même en prenant soin de changer la notation un peu maladroite pour ce qu'on s'apprête à faire.

\begin{equation*}
    U(t,t_0) = \mathbb{I} -i\int_{t_0}^{t}H(t_1)U(t_1,t_0)dt_1 = \mathbb{I} -i\int_{t_0}^{t}H(t_1)\left(\mathbb{I} - i\int_{t_0}^{t_1}H(t_2)U(t_2,t_0)dt_2\right)dt_1
\end{equation*}
\begin{equation*}
    = \mathbb{I} -i\int_{t_0}^{t}H(t_1)dt_1 - \int_{t_0}^{t}H(t_1)dt_1\int_{t_0}^{t_1}H(t_2)U(t_2,t_0)dt_2
\end{equation*}

En continuant ainsi à l'infini, on obtient comme relation générale que

\begin{equation*}
    U(t,t_0) = \mathbb{I} + \sum_{n=1}^{\infty}(-i)^n \int_{t_0}^{t}H(t_1)dt_1\int_{t_0}^{t_1}H(t_2)dt_2 \ ... \int_{t_0}^{t_{n-1}}H(t_n)dt_n \ ...
\end{equation*}
\begin{equation}
    = \sum_{n=0}^{\infty}(-i)^n \int_{t_0}^{t}H(t_1)dt_1\int_{t_0}^{t_1}H(t_2)dt_2 \ ... \int_{t_0}^{t_{n-1}}H(t_n)dt_n \ ...
\end{equation}

ce qu'on appelle une série de Dyson. Par construction, les variables d'intégration respectent $t_1 \geq t_2 \geq ... \geq t_n \geq ...$, mais la dépendance des $t_i$ dans les bornes d'intégration est agaçante et il peut être difficile de voir comment (1.5) se réduit à (1.4) ou à (1.3) avec un changement approprié des conditions. De plus, il est hautement non-trivial de montrer que (1.5) converge. 

On introduit alors l'opérateur de produit chronologique $T$ qui réordonne un produit matriciel de manière à ce que l'argument en temps des matrices dans le produit soit décroissant de la gauche vers la droite. Autrement dit, dans notre cas,

\begin{equation}
    T[H(t_1)H(t_2)...H(t_n)] = H(t_{i_1})H(t_{i_2})...H(t_{i_n}) \text{ où } t_{i_1} \geq t_{i_2} \geq ... \geq t_{i_n}
\end{equation}

Évidemment, si tous les hamiltoniens commutent entre eux, alors $T$ n'a aucun effet. Pour clarifier l'utilisation de l'opérateur de produit chronologique, on revient à (1.5) en s'attardant à $J_2$ l'intégrale du terme $n=2$ de la somme.

\begin{equation*}
    J_2 = \int_{t_0}^{t}H(t_1)dt_1\int_{t_0}^{t_1}H(t_2)dt_2 = \int_{t_0}^{t}dt_1\int_{t_0}^{t_1}H(t_1)H(t_2)dt_2
\end{equation*}

Il est important de conserver le même ordre pour la multiplication matricielle, car par hypothèse les hamiltoniens ne commutent pas. Dans cette dernière équation, l'ordre d'intégration fait en sorte que $t_1 \geq t_2$, ce qu'on peut voir en représentant la région d'intégration (qui est la moitié de l'aire d'un carré de côté $t$). Ainsi, on peut directement incorporer l'opérateur de produit chronologique dans $J_2$.

\begin{equation*}
    J_2 = \int_{t_0}^{t}dt_1\int_{t_0}^{t_1}H(t_1)H(t_2)dt_2 = \int_{t_0}^{t}dt_1\int_{t_0}^{t_1}T\left[H(t_1)H(t_2)\right]dt_2
\end{equation*}

Sans changer la valeur de $J_2$, on peut changer l'ordre d'intégration de la manière suivante :

\begin{equation*}
    J_2 = \int_{t_0}^{t}dt_2\int_{t_2}^{t}H(t_1)H(t_2)dt_1
\end{equation*}

En représentant cette nouvelle région d'intégration, on voit qu'elle reste la même sauf que maintenant l'intégration se fait "horizontalement" au lieu de "verticalement". On peut ensuite procéder à un changement de variables (qui ne change toujours pas la valeur de $J_2$ s'il est fait correctement) où $t_1 \Leftrightarrow t_2$. 

\begin{equation*}
    J_2 = \int_{t_0}^{t}dt_1\int_{t_1}^{t}H(t_2)H(t_1)dt_2
\end{equation*}

La région d'intégration fait alors une réflexion par rapport à l'axe de la droite $t_1=t_2$ et correspond alors à la moitié restante de l'aire du carré de côté $t$. Dans ce cas, $t_2 \geq t_1$ et on écrit 

\begin{equation*}
    J_2 = \int_{t_0}^{t}dt_1\int_{t_1}^{t}T\left[H(t_1)H(t_2)\right]dt_2
\end{equation*}

Au final, on vient de trouver 2 formes différentes pour $J_2$ qu'on résume ici :

\begin{equation}
    J_2 = \int_{t_0}^{t}dt_1\int_{t_0}^{t_1}T\left[H(t_1)H(t_2)\right]dt_2 = \int_{t_0}^{t}dt_1\int_{t_1}^{t}T\left[H(t_1)H(t_2)\right]dt_2
\end{equation}

On remarque qu'en sommant ensemble chacune des formes, on peut avoir une formule pour $J_2$ où les bornes d'intégration ne dépendent plus des $t_i$. On peut ensuite l'incorporer dans (1.5).

\begin{equation*}
    2J_2 = \int_{t_0}^{t}dt_1\int_{t_0}^{t_1}T\left[H(t_1)H(t_2)\right]dt_2 + \int_{t_0}^{t}dt_1\int_{t_1}^{t}T\left[H(t_1)H(t_2)\right]dt_2
\end{equation*}
\begin{equation*}
    = \int_{t_0}^{t}dt_1\left(\int_{t_0}^{t_1}T\left[H(t_1)H(t_2)\right]dt_2 + \int_{t_1}^{t}T\left[H(t_1)H(t_2)\right]dt_2\right)
\end{equation*}
\begin{equation}
    = \int_{t_0}^{t}dt_1\int_{t_0}^{t}T\left[H(t_1)H(t_2)\right]dt_2 \implies J_2 = \frac{1}{2}\int_{t_0}^{t}dt_1\int_{t_0}^{t}T\left[H(t_1)H(t_2)\right]dt_2 
\end{equation}

En général, pour $J_n$, il existera $n!$ façons différentes de l'écrire (pour les $n!$ façons d'organiser les $n$ hamiltoniens qui seront présents $\rightarrow$ les $n!$ changements de variables possibles). Par la suite, en sommant ces $n!$ équations, toutes les dépendances sur les $t_i$ partiront et la somme correspondra à $n!J_n$. Finalement, on isole $J_n$ pour obtenir 

\begin{equation}
    J_n = \frac{1}{n!}\int_{t_0}^{t}dt_1\int_{t_0}^{t}dt_2...\int_{t_0}^{t}dt_n T \left[H(t_1)H(t_2)...H(t_n)\right]
\end{equation}

qu'on ajoute dans (1.5) donnant ainsi

\begin{equation}
    U(t,t_0) = \sum_{n=0}^{\infty}\frac{(-i)^n}{n!}\int_{t_0}^{t}dt_1\int_{t_0}^{t}dt_2...\int_{t_0}^{t}dt_n T \left[H(t_1)H(t_2)...H(t_n)\right]
\end{equation}

De (1.10), si les hamiltoniens commutent (donc que $T$ ne fait rien), on voit qu'on peut retomber sur (1.4).

\begin{equation*}
    U(t,t_0) = \sum_{n=0}^{\infty}\frac{(-i)^n}{n!}\int_{t_0}^{t}dt_1\int_{t_0}^{t}dt_2...\int_{t_0}^{t}dt_n H(t_1)H(t_2)...H(t_n) = \sum_{n=0}^{\infty}\frac{(-i)^n}{n!}\left(\int_{t_0}^{t}H(t^{'})dt^{'}\right)^n
\end{equation*}
\begin{equation*}
    = \sum_{n=0}^{\infty}\frac{1}{n!}\left(-i\int_{t_0}^{t}H(t^{'})dt^{'}\right)^n = e^{-i\int_{t_0}^{t}H(t^{'})dt^{'}}
\end{equation*}

Dans les ouvrages, on utilise plutôt l'écriture

\begin{equation}
    U(t,t_0) = Te^{-i\int_{t_0}^{t}H(t^{'})dt^{'}} = \sum_{n=0}^{\infty}\frac{(-i)^n}{n!}\int_{t_0}^{t}dt_1\int_{t_0}^{t}dt_2...\int_{t_0}^{t}dt_n T \left[H(t_1)H(t_2)...H(t_n)\right]
\end{equation}

comme étant la formule la plus générale pour résoudre (1.2).

\subsection{Système ouvert}
Pour l'instant, on s'est intéressé à l'équation de Schrödinger pour un système fermé, c'est-à-dire un système isolé de l'environnement. Cependant, un système quantique peut aussi intéragir avec son environnement et on qualifie le système comme étant ouvert. Dans ce cas, à la place d'un vecteur d'état (un ket), on a un ensemble statistique quantique (une distribution statistique d'états quantiques) qu'on représente par l'entremise d'une matrice densité $\rho(t)$.

\begin{equation}
    \rho(t) = \sum_{j}^{}p_j \ket{\psi_j(t)}\bra{\psi_j(t)} = \sum_{j}^{}p_j U(t,t_0)\ket{\psi_j(t_0)}\bra{\psi_j(t_0)}U^{\dagger}(t,t_0) = U(t,t_0)\rho(t_0)U^{\dagger}(t,t_0)
\end{equation}

Les $p_j$ correspondent aux probabilités de chaque état dans la distribution, donc $\sum_{j}^{}p_j = 1$ nécessairement. Par ailleurs, pour être physique, une matrice densité doit aussi être hermitienne ($\rho = \rho^{\dagger}$) et positive semi-définie ($\bra{\psi}\rho\ket{\psi} \geq 0$). Ces conditions assurent que les $p_j$ sont réels et $\geq 0$ (comme ça devrait être le cas pour des probabilités).












